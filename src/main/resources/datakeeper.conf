spark {
  app.name = "datakeeper"
  serializer = "org.apache.spark.serializer.KryoSerializer"
}

kafka {
  host = "kafka-headless"
  port = 9092
  url = ""${kafka.host}":"${kafka.port}""
  topic = "datakeeper-test-topic"
  groupId = "datakeeper-test-cgroup"

  key.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
  value.deserializer = "io.confluent.kafka.serializers.KafkaAvroDeserializer"

  key.serializer = "org.apache.kafka.common.serialization.StringSerializer"
  value.serializer = "io.confluent.kafka.serializers.KafkaAvroSerializer"

  schema.registry.url = "http://cp-schema-registry:8081"

  autocommit = false
  offsetMode = "earliest"
}

zookeeper {
  url = "zookeeper-headless:2181"
  session-timeout.ms = 20000
  connection-timeout.ms = 8000
}

fs {
  url = "hdfs://namenode:8020"
  dir = "/tmp/datakeeper"
  format = "parquet"
  numOfOutputFiles = 1 #calculated ad-hoc
}

columns {
  identity = ["id"]
  partitioning = ["group_id"]
  sorting = ["last_visited"] #mind sorting fields order
}

execution {
  maxMessagesPerPartition: 100 #for test only
  partitionsParallelism: 2 #calculated ad-hoc
}

hive {
  tableName = "test.datakeeper"
}
